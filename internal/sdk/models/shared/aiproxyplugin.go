// Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT.

package shared

import (
	"encoding/json"
	"fmt"
	"github.com/kong/terraform-provider-kong-gateway/internal/sdk/internal/utils"
	"github.com/kong/terraform-provider-kong-gateway/internal/sdk/types"
)

// AIProxyPluginParamLocation - Specify whether the 'param_name' and 'param_value' options go in a query string, or the POST form/JSON body.
type AIProxyPluginParamLocation string

const (
	AIProxyPluginParamLocationQuery AIProxyPluginParamLocation = "query"
	AIProxyPluginParamLocationBody  AIProxyPluginParamLocation = "body"
)

func (e AIProxyPluginParamLocation) ToPointer() *AIProxyPluginParamLocation {
	return &e
}
func (e *AIProxyPluginParamLocation) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "query":
		fallthrough
	case "body":
		*e = AIProxyPluginParamLocation(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AIProxyPluginParamLocation: %v", v)
	}
}

type AIProxyPluginAuth struct {
	// If azure_use_managed_identity is set to true, and you need to use a different user-assigned identity for this LLM instance, set the client ID.
	AzureClientID *string `json:"azure_client_id,omitempty"`
	// If azure_use_managed_identity is set to true, and you need to use a different user-assigned identity for this LLM instance, set the client secret.
	AzureClientSecret *string `json:"azure_client_secret,omitempty"`
	// If azure_use_managed_identity is set to true, and you need to use a different user-assigned identity for this LLM instance, set the tenant ID.
	AzureTenantID *string `json:"azure_tenant_id,omitempty"`
	// Set true to use the Azure Cloud Managed Identity (or user-assigned identity) to authenticate with Azure-provider models.
	AzureUseManagedIdentity *bool `json:"azure_use_managed_identity,omitempty"`
	// If AI model requires authentication via Authorization or API key header, specify its name here.
	HeaderName *string `json:"header_name,omitempty"`
	// Specify the full auth header value for 'header_name', for example 'Bearer key' or just 'key'.
	HeaderValue *string `json:"header_value,omitempty"`
	// Specify whether the 'param_name' and 'param_value' options go in a query string, or the POST form/JSON body.
	ParamLocation *AIProxyPluginParamLocation `json:"param_location,omitempty"`
	// If AI model requires authentication via query parameter, specify its name here.
	ParamName *string `json:"param_name,omitempty"`
	// Specify the full parameter value for 'param_name'.
	ParamValue *string `json:"param_value,omitempty"`
}

func (o *AIProxyPluginAuth) GetAzureClientID() *string {
	if o == nil {
		return nil
	}
	return o.AzureClientID
}

func (o *AIProxyPluginAuth) GetAzureClientSecret() *string {
	if o == nil {
		return nil
	}
	return o.AzureClientSecret
}

func (o *AIProxyPluginAuth) GetAzureTenantID() *string {
	if o == nil {
		return nil
	}
	return o.AzureTenantID
}

func (o *AIProxyPluginAuth) GetAzureUseManagedIdentity() *bool {
	if o == nil {
		return nil
	}
	return o.AzureUseManagedIdentity
}

func (o *AIProxyPluginAuth) GetHeaderName() *string {
	if o == nil {
		return nil
	}
	return o.HeaderName
}

func (o *AIProxyPluginAuth) GetHeaderValue() *string {
	if o == nil {
		return nil
	}
	return o.HeaderValue
}

func (o *AIProxyPluginAuth) GetParamLocation() *AIProxyPluginParamLocation {
	if o == nil {
		return nil
	}
	return o.ParamLocation
}

func (o *AIProxyPluginAuth) GetParamName() *string {
	if o == nil {
		return nil
	}
	return o.ParamName
}

func (o *AIProxyPluginAuth) GetParamValue() *string {
	if o == nil {
		return nil
	}
	return o.ParamValue
}

type AIProxyPluginLogging struct {
	// If enabled, will log the request and response body into the Kong log plugin(s) output.
	LogPayloads *bool `json:"log_payloads,omitempty"`
	// If enabled and supported by the driver, will add model usage and token metrics into the Kong log plugin(s) output.
	LogStatistics *bool `json:"log_statistics,omitempty"`
}

func (o *AIProxyPluginLogging) GetLogPayloads() *bool {
	if o == nil {
		return nil
	}
	return o.LogPayloads
}

func (o *AIProxyPluginLogging) GetLogStatistics() *bool {
	if o == nil {
		return nil
	}
	return o.LogStatistics
}

// AIProxyPluginLlama2Format - If using llama2 provider, select the upstream message format.
type AIProxyPluginLlama2Format string

const (
	AIProxyPluginLlama2FormatRaw    AIProxyPluginLlama2Format = "raw"
	AIProxyPluginLlama2FormatOpenai AIProxyPluginLlama2Format = "openai"
	AIProxyPluginLlama2FormatOllama AIProxyPluginLlama2Format = "ollama"
)

func (e AIProxyPluginLlama2Format) ToPointer() *AIProxyPluginLlama2Format {
	return &e
}
func (e *AIProxyPluginLlama2Format) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "raw":
		fallthrough
	case "openai":
		fallthrough
	case "ollama":
		*e = AIProxyPluginLlama2Format(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AIProxyPluginLlama2Format: %v", v)
	}
}

// AIProxyPluginMistralFormat - If using mistral provider, select the upstream message format.
type AIProxyPluginMistralFormat string

const (
	AIProxyPluginMistralFormatOpenai AIProxyPluginMistralFormat = "openai"
	AIProxyPluginMistralFormatOllama AIProxyPluginMistralFormat = "ollama"
)

func (e AIProxyPluginMistralFormat) ToPointer() *AIProxyPluginMistralFormat {
	return &e
}
func (e *AIProxyPluginMistralFormat) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "openai":
		fallthrough
	case "ollama":
		*e = AIProxyPluginMistralFormat(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AIProxyPluginMistralFormat: %v", v)
	}
}

// AIProxyPluginOptions - Key/value settings for the model
type AIProxyPluginOptions struct {
	// Defines the schema/API version, if using Anthropic provider.
	AnthropicVersion *string `json:"anthropic_version,omitempty"`
	// 'api-version' for Azure OpenAI instances.
	AzureAPIVersion *string `json:"azure_api_version,omitempty"`
	// Deployment ID for Azure OpenAI instances.
	AzureDeploymentID *string `json:"azure_deployment_id,omitempty"`
	// Instance name for Azure OpenAI hosted models.
	AzureInstance *string `json:"azure_instance,omitempty"`
	// If using llama2 provider, select the upstream message format.
	Llama2Format *AIProxyPluginLlama2Format `json:"llama2_format,omitempty"`
	// Defines the max_tokens, if using chat or completion models.
	MaxTokens *int64 `json:"max_tokens,omitempty"`
	// If using mistral provider, select the upstream message format.
	MistralFormat *AIProxyPluginMistralFormat `json:"mistral_format,omitempty"`
	// Defines the matching temperature, if using chat or completion models.
	Temperature *float64 `json:"temperature,omitempty"`
	// Defines the top-k most likely tokens, if supported.
	TopK *int64 `json:"top_k,omitempty"`
	// Defines the top-p probability mass, if supported.
	TopP *float64 `json:"top_p,omitempty"`
	// Manually specify or override the AI operation path, used when e.g. using the 'preserve' route_type.
	UpstreamPath *string `json:"upstream_path,omitempty"`
	// Manually specify or override the full URL to the AI operation endpoints, when calling (self-)hosted models, or for running via a private endpoint.
	UpstreamURL *string `json:"upstream_url,omitempty"`
}

func (o *AIProxyPluginOptions) GetAnthropicVersion() *string {
	if o == nil {
		return nil
	}
	return o.AnthropicVersion
}

func (o *AIProxyPluginOptions) GetAzureAPIVersion() *string {
	if o == nil {
		return nil
	}
	return o.AzureAPIVersion
}

func (o *AIProxyPluginOptions) GetAzureDeploymentID() *string {
	if o == nil {
		return nil
	}
	return o.AzureDeploymentID
}

func (o *AIProxyPluginOptions) GetAzureInstance() *string {
	if o == nil {
		return nil
	}
	return o.AzureInstance
}

func (o *AIProxyPluginOptions) GetLlama2Format() *AIProxyPluginLlama2Format {
	if o == nil {
		return nil
	}
	return o.Llama2Format
}

func (o *AIProxyPluginOptions) GetMaxTokens() *int64 {
	if o == nil {
		return nil
	}
	return o.MaxTokens
}

func (o *AIProxyPluginOptions) GetMistralFormat() *AIProxyPluginMistralFormat {
	if o == nil {
		return nil
	}
	return o.MistralFormat
}

func (o *AIProxyPluginOptions) GetTemperature() *float64 {
	if o == nil {
		return nil
	}
	return o.Temperature
}

func (o *AIProxyPluginOptions) GetTopK() *int64 {
	if o == nil {
		return nil
	}
	return o.TopK
}

func (o *AIProxyPluginOptions) GetTopP() *float64 {
	if o == nil {
		return nil
	}
	return o.TopP
}

func (o *AIProxyPluginOptions) GetUpstreamPath() *string {
	if o == nil {
		return nil
	}
	return o.UpstreamPath
}

func (o *AIProxyPluginOptions) GetUpstreamURL() *string {
	if o == nil {
		return nil
	}
	return o.UpstreamURL
}

// AIProxyPluginProvider - AI provider request format - Kong translates requests to and from the specified backend compatible formats.
type AIProxyPluginProvider string

const (
	AIProxyPluginProviderOpenai    AIProxyPluginProvider = "openai"
	AIProxyPluginProviderAzure     AIProxyPluginProvider = "azure"
	AIProxyPluginProviderAnthropic AIProxyPluginProvider = "anthropic"
	AIProxyPluginProviderCohere    AIProxyPluginProvider = "cohere"
	AIProxyPluginProviderMistral   AIProxyPluginProvider = "mistral"
	AIProxyPluginProviderLlama2    AIProxyPluginProvider = "llama2"
)

func (e AIProxyPluginProvider) ToPointer() *AIProxyPluginProvider {
	return &e
}
func (e *AIProxyPluginProvider) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "openai":
		fallthrough
	case "azure":
		fallthrough
	case "anthropic":
		fallthrough
	case "cohere":
		fallthrough
	case "mistral":
		fallthrough
	case "llama2":
		*e = AIProxyPluginProvider(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AIProxyPluginProvider: %v", v)
	}
}

type AIProxyPluginModel struct {
	// Model name to execute.
	Name *string `json:"name,omitempty"`
	// Key/value settings for the model
	Options *AIProxyPluginOptions `json:"options,omitempty"`
	// AI provider request format - Kong translates requests to and from the specified backend compatible formats.
	Provider *AIProxyPluginProvider `json:"provider,omitempty"`
}

func (o *AIProxyPluginModel) GetName() *string {
	if o == nil {
		return nil
	}
	return o.Name
}

func (o *AIProxyPluginModel) GetOptions() *AIProxyPluginOptions {
	if o == nil {
		return nil
	}
	return o.Options
}

func (o *AIProxyPluginModel) GetProvider() *AIProxyPluginProvider {
	if o == nil {
		return nil
	}
	return o.Provider
}

// AIProxyPluginResponseStreaming - Whether to 'optionally allow', 'deny', or 'always' (force) the streaming of answers via server sent events.
type AIProxyPluginResponseStreaming string

const (
	AIProxyPluginResponseStreamingAllow  AIProxyPluginResponseStreaming = "allow"
	AIProxyPluginResponseStreamingDeny   AIProxyPluginResponseStreaming = "deny"
	AIProxyPluginResponseStreamingAlways AIProxyPluginResponseStreaming = "always"
)

func (e AIProxyPluginResponseStreaming) ToPointer() *AIProxyPluginResponseStreaming {
	return &e
}
func (e *AIProxyPluginResponseStreaming) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "allow":
		fallthrough
	case "deny":
		fallthrough
	case "always":
		*e = AIProxyPluginResponseStreaming(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AIProxyPluginResponseStreaming: %v", v)
	}
}

// AIProxyPluginRouteType - The model's operation implementation, for this provider. Set to `preserve` to pass through without transformation.
type AIProxyPluginRouteType string

const (
	AIProxyPluginRouteTypeLlmV1Chat        AIProxyPluginRouteType = "llm/v1/chat"
	AIProxyPluginRouteTypeLlmV1Completions AIProxyPluginRouteType = "llm/v1/completions"
	AIProxyPluginRouteTypePreserve         AIProxyPluginRouteType = "preserve"
)

func (e AIProxyPluginRouteType) ToPointer() *AIProxyPluginRouteType {
	return &e
}
func (e *AIProxyPluginRouteType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "llm/v1/chat":
		fallthrough
	case "llm/v1/completions":
		fallthrough
	case "preserve":
		*e = AIProxyPluginRouteType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AIProxyPluginRouteType: %v", v)
	}
}

type AIProxyPluginConfig struct {
	Auth    *AIProxyPluginAuth    `json:"auth,omitempty"`
	Logging *AIProxyPluginLogging `json:"logging,omitempty"`
	Model   *AIProxyPluginModel   `json:"model,omitempty"`
	// Whether to 'optionally allow', 'deny', or 'always' (force) the streaming of answers via server sent events.
	ResponseStreaming *AIProxyPluginResponseStreaming `json:"response_streaming,omitempty"`
	// The model's operation implementation, for this provider. Set to `preserve` to pass through without transformation.
	RouteType *AIProxyPluginRouteType `json:"route_type,omitempty"`
}

func (o *AIProxyPluginConfig) GetAuth() *AIProxyPluginAuth {
	if o == nil {
		return nil
	}
	return o.Auth
}

func (o *AIProxyPluginConfig) GetLogging() *AIProxyPluginLogging {
	if o == nil {
		return nil
	}
	return o.Logging
}

func (o *AIProxyPluginConfig) GetModel() *AIProxyPluginModel {
	if o == nil {
		return nil
	}
	return o.Model
}

func (o *AIProxyPluginConfig) GetResponseStreaming() *AIProxyPluginResponseStreaming {
	if o == nil {
		return nil
	}
	return o.ResponseStreaming
}

func (o *AIProxyPluginConfig) GetRouteType() *AIProxyPluginRouteType {
	if o == nil {
		return nil
	}
	return o.RouteType
}

type AIProxyPluginProtocols string

const (
	AIProxyPluginProtocolsGrpc           AIProxyPluginProtocols = "grpc"
	AIProxyPluginProtocolsGrpcs          AIProxyPluginProtocols = "grpcs"
	AIProxyPluginProtocolsHTTP           AIProxyPluginProtocols = "http"
	AIProxyPluginProtocolsHTTPS          AIProxyPluginProtocols = "https"
	AIProxyPluginProtocolsTCP            AIProxyPluginProtocols = "tcp"
	AIProxyPluginProtocolsTLS            AIProxyPluginProtocols = "tls"
	AIProxyPluginProtocolsTLSPassthrough AIProxyPluginProtocols = "tls_passthrough"
	AIProxyPluginProtocolsUDP            AIProxyPluginProtocols = "udp"
	AIProxyPluginProtocolsWs             AIProxyPluginProtocols = "ws"
	AIProxyPluginProtocolsWss            AIProxyPluginProtocols = "wss"
)

func (e AIProxyPluginProtocols) ToPointer() *AIProxyPluginProtocols {
	return &e
}
func (e *AIProxyPluginProtocols) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "grpc":
		fallthrough
	case "grpcs":
		fallthrough
	case "http":
		fallthrough
	case "https":
		fallthrough
	case "tcp":
		fallthrough
	case "tls":
		fallthrough
	case "tls_passthrough":
		fallthrough
	case "udp":
		fallthrough
	case "ws":
		fallthrough
	case "wss":
		*e = AIProxyPluginProtocols(v)
		return nil
	default:
		return fmt.Errorf("invalid value for AIProxyPluginProtocols: %v", v)
	}
}

// AIProxyPluginConsumer - If set, the plugin will activate only for requests where the specified has been authenticated. (Note that some plugins can not be restricted to consumers this way.). Leave unset for the plugin to activate regardless of the authenticated Consumer.
type AIProxyPluginConsumer struct {
	ID *string `json:"id,omitempty"`
}

func (o *AIProxyPluginConsumer) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

type AIProxyPluginConsumerGroup struct {
	ID *string `json:"id,omitempty"`
}

func (o *AIProxyPluginConsumerGroup) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

// AIProxyPluginRoute - If set, the plugin will only activate when receiving requests via the specified route. Leave unset for the plugin to activate regardless of the Route being used.
type AIProxyPluginRoute struct {
	ID *string `json:"id,omitempty"`
}

func (o *AIProxyPluginRoute) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

// AIProxyPluginService - If set, the plugin will only activate when receiving requests via one of the routes belonging to the specified Service. Leave unset for the plugin to activate regardless of the Service being matched.
type AIProxyPluginService struct {
	ID *string `json:"id,omitempty"`
}

func (o *AIProxyPluginService) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

type AIProxyPlugin struct {
	Config *AIProxyPluginConfig `json:"config,omitempty"`
	// Unix epoch when the resource was created.
	CreatedAt *int64 `json:"created_at,omitempty"`
	// Whether the plugin is applied.
	Enabled      *bool   `json:"enabled,omitempty"`
	ID           *string `json:"id,omitempty"`
	InstanceName *string `json:"instance_name,omitempty"`
	name         *string `const:"ai-proxy" json:"name,omitempty"`
	Ordering     any     `json:"ordering,omitempty"`
	// A list of the request protocols that will trigger this plugin. The default value, as well as the possible values allowed on this field, may change depending on the plugin type. For example, plugins that only work in stream mode will only support `"tcp"` and `"tls"`.
	Protocols []AIProxyPluginProtocols `json:"protocols,omitempty"`
	// An optional set of strings associated with the Plugin for grouping and filtering.
	Tags []string `json:"tags,omitempty"`
	// Unix epoch when the resource was last updated.
	UpdatedAt *int64 `json:"updated_at,omitempty"`
	// If set, the plugin will activate only for requests where the specified has been authenticated. (Note that some plugins can not be restricted to consumers this way.). Leave unset for the plugin to activate regardless of the authenticated Consumer.
	Consumer      *AIProxyPluginConsumer      `json:"consumer,omitempty"`
	ConsumerGroup *AIProxyPluginConsumerGroup `json:"consumer_group,omitempty"`
	// If set, the plugin will only activate when receiving requests via the specified route. Leave unset for the plugin to activate regardless of the Route being used.
	Route *AIProxyPluginRoute `json:"route,omitempty"`
	// If set, the plugin will only activate when receiving requests via one of the routes belonging to the specified Service. Leave unset for the plugin to activate regardless of the Service being matched.
	Service *AIProxyPluginService `json:"service,omitempty"`
}

func (a AIProxyPlugin) MarshalJSON() ([]byte, error) {
	return utils.MarshalJSON(a, "", false)
}

func (a *AIProxyPlugin) UnmarshalJSON(data []byte) error {
	if err := utils.UnmarshalJSON(data, &a, "", false, false); err != nil {
		return err
	}
	return nil
}

func (o *AIProxyPlugin) GetConfig() *AIProxyPluginConfig {
	if o == nil {
		return nil
	}
	return o.Config
}

func (o *AIProxyPlugin) GetCreatedAt() *int64 {
	if o == nil {
		return nil
	}
	return o.CreatedAt
}

func (o *AIProxyPlugin) GetEnabled() *bool {
	if o == nil {
		return nil
	}
	return o.Enabled
}

func (o *AIProxyPlugin) GetID() *string {
	if o == nil {
		return nil
	}
	return o.ID
}

func (o *AIProxyPlugin) GetInstanceName() *string {
	if o == nil {
		return nil
	}
	return o.InstanceName
}

func (o *AIProxyPlugin) GetName() *string {
	return types.String("ai-proxy")
}

func (o *AIProxyPlugin) GetOrdering() any {
	if o == nil {
		return nil
	}
	return o.Ordering
}

func (o *AIProxyPlugin) GetProtocols() []AIProxyPluginProtocols {
	if o == nil {
		return nil
	}
	return o.Protocols
}

func (o *AIProxyPlugin) GetTags() []string {
	if o == nil {
		return nil
	}
	return o.Tags
}

func (o *AIProxyPlugin) GetUpdatedAt() *int64 {
	if o == nil {
		return nil
	}
	return o.UpdatedAt
}

func (o *AIProxyPlugin) GetConsumer() *AIProxyPluginConsumer {
	if o == nil {
		return nil
	}
	return o.Consumer
}

func (o *AIProxyPlugin) GetConsumerGroup() *AIProxyPluginConsumerGroup {
	if o == nil {
		return nil
	}
	return o.ConsumerGroup
}

func (o *AIProxyPlugin) GetRoute() *AIProxyPluginRoute {
	if o == nil {
		return nil
	}
	return o.Route
}

func (o *AIProxyPlugin) GetService() *AIProxyPluginService {
	if o == nil {
		return nil
	}
	return o.Service
}
